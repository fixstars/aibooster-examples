apiVersion: v1
kind: ConfigMap
metadata:
  name: training-script
data:
  train_mnist.py: |
    #!/usr/bin/env python
    """MNIST training script for OMP_NUM_THREADS and num_workers tuning example."""

    import argparse
    import os
    import time
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    from torchvision import datasets, transforms


    # Define a simple CNN model
    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.conv1 = nn.Conv2d(1, 4, 3, 1)
            self.conv2 = nn.Conv2d(4, 8, 3, 1)
            self.dropout1 = nn.Dropout(0.25)
            self.dropout2 = nn.Dropout(0.5)
            self.fc1 = nn.Linear(1152, 16)
            self.fc2 = nn.Linear(16, 10)

        def forward(self, x):
            x = self.conv1(x)
            x = F.relu(x)
            x = self.conv2(x)
            x = F.relu(x)
            x = F.max_pool2d(x, 2)
            x = self.dropout1(x)
            x = torch.flatten(x, 1)
            x = self.fc1(x)
            x = F.relu(x)
            x = self.dropout2(x)
            x = self.fc2(x)
            output = F.log_softmax(x, dim=1)
            return output


    def main():
        # Parse command line arguments
        parser = argparse.ArgumentParser(description='MNIST training with tunable parameters')
        parser.add_argument('--num-workers', type=int, default=0,
                            help='Number of data loading workers (default: 0)')
        args = parser.parse_args()

        # Get OMP_NUM_THREADS from environment (use system default if not set)
        if 'OMP_NUM_THREADS' in os.environ:
            omp_threads = int(os.environ['OMP_NUM_THREADS'])
            print(f'Using OMP_NUM_THREADS: {omp_threads}')
            torch.set_num_threads(omp_threads)
        else:
            print('OMP_NUM_THREADS not set, using PyTorch default')

        print(f'Using DataLoader num_workers: {args.num_workers}')

        # Load MNIST dataset
        print('Loading MNIST dataset...')
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])

        dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)
        train_loader = torch.utils.data.DataLoader(
            dataset, batch_size=32, shuffle=True, num_workers=args.num_workers
        )

        # Initialize model, optimizer
        model = Net()
        optimizer = optim.SGD(model.parameters(), lr=0.01)

        # Start timing
        start_time = time.time()

        # Train for a few epochs
        print('Training model...')
        model.train()
        for epoch in range(2):
            epoch_start_time = time.time()
            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                output = model(data)
                loss = F.nll_loss(output, target)
                loss.backward()
                optimizer.step()
            epoch_elapsed_time = time.time() - epoch_start_time
            print(f'Epoch {epoch+1} completed in {epoch_elapsed_time:.4f} seconds')

        # End timing
        elapsed_time = time.time() - start_time

        print(f'Training completed')
        print(f'Elapsed time: {elapsed_time:.4f} seconds')


    if __name__ == "__main__":
        main()
---
apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  generateName: job-training-
spec:
  pytorchReplicaSpecs:
    Worker:
      replicas: 1
      restartPolicy: Never
      template:
        spec:
          containers:
            - name: pytorch
              image: pytorch/pytorch:1.13.1-cuda11.6-cudnn8-runtime
              command:
                - sh
                - -c
                - |
                  python /scripts/train_mnist.py
              resources:
                limits:
                  cpu: "2"
                  memory: "1Gi"
              volumeMounts:
                - name: training-script
                  mountPath: /scripts
          volumes:
            - name: training-script
              configMap:
                name: training-script
